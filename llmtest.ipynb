{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a1daa-9b50-4206-b8a7-0f78e68fad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def chat_response(input):\n",
    "    response: ChatResponse = chat(model='gemma3n:e2b', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': input,\n",
    "      },\n",
    "    ])\n",
    "\n",
    "    return response['message']['content'].strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd6ed4-1df8-4c7a-8a05-7660117c1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "###First test case\n",
    "###Sentiment Analysis, Task Fidelity, Exact Match\n",
    "def test_sentiment_002():\n",
    "    assert chat_response(f\"Classify this as 'positive', 'negative', 'neutral', or 'mixed': The movie is not good. Just return the emotion\") == \"negative\"\n",
    "    \n",
    "\n",
    "ipytest.run()\n",
    "    \n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85258fc6-c07b-41ab-a788-2669ad2699b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Second test case\n",
    "####Consistency, FAQ chatbot, Cosine similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "faq_variations = [\n",
    "    {\"questions\": [\"What's your return policy?\", \"How can I return an item?\", \"Wut's yur retrn polcy?\"], \"answer\": \"Our return policy allows...\"},  # Edge case: Typos\n",
    "    {\"questions\": [\"I bought something last week, and it's not really what I expected, so I was wondering if maybe I could possibly return it?\", \"I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I'm wondering what exactly is your current policy?\"], \"answer\": \"Our return policy allows...\"},  # Edge case: Long, rambling question\n",
    "    {\"questions\": [\"I'm Jane's cousin, and she said you guys have great customer service. Can I return this?\", \"Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they're right! What is the return window for a jacket?\"], \"answer\": \"Our return policy allows...\"},  # Edge case: Irrelevant info\n",
    "    # ... 47 more FAQs\n",
    "]\n",
    "\n",
    "def check_cosine_similarity(outputs):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = np.array([model.encode(output) for output in outputs])\n",
    "    print(\"Embeddings ready\")\n",
    "    cosine_similarities = np.dot(embeddings, embeddings.T) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).T)\n",
    "    print(\"Got similarity score\")\n",
    "    return np.mean(cosine_similarities)\n",
    "\n",
    "def sim_score():\n",
    "    score_list = []\n",
    "    for faq in faq_variations:\n",
    "        outputs = [chat_response(question) for question in faq[\"questions\"]]\n",
    "        print(\"Got chat response\")\n",
    "        similarity_score = check_cosine_similarity(outputs)\n",
    "        score_list.append(similarity_score)\n",
    "    return score_list\n",
    "\n",
    "score_list = sim_score()\n",
    "@pytest.mark.parametrize(\"score\",score_list)\n",
    "def test_faq_002(score):\n",
    "    if score > 0.7:\n",
    "        assert True\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24957a3-3b27-4d00-aa3a-a69055b65cfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21335a79-8b2d-4d21-95d1-d0163caa47ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2f68d-df8a-4c5e-a875-0ce3dee67fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7be37-2970-4e5e-a757-4573e0110bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49a351-2f87-4c44-98d2-7d76fec429cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Third test case\n",
    "####Summarization, Relevance, Coherance, ROUGE-L\n",
    "from rouge import Rouge\n",
    "\n",
    "articles = [\n",
    "    {\"text\": \"In a groundbreaking study, researchers at MIT...\", \"summary\": \"MIT scientists discover a new antibiotic...\"},\n",
    "    {\"text\": \"Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...\", \"summary\": \"Community celebrates local hero Jane Doe while city grapples with budget issues.\"},  # Edge case: Multi-topic\n",
    "    {\"text\": \"You won't believe what this celebrity did! ... extensive charity work ...\", \"summary\": \"Celebrity's extensive charity work surprises fans\"},  # Edge case: Misleading title\n",
    "    # ... 197 more articles\n",
    "]\n",
    "\n",
    "def evaluate_rouge_l(model_output, true_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(model_output, true_summary)\n",
    "    return scores[0]['rouge-l']['f']  # ROUGE-L F1 score\n",
    "\n",
    "def get_relevance_score():\n",
    "    outputs = [chat_response(f\"Summarize this article in 1-2 sentences:\\n\\n{article['text']}\") for article in articles]\n",
    "    relevance_scores = [evaluate_rouge_l(output, article['summary']) for output, article in zip(outputs, articles)]\n",
    "    av_score = sum(relevance_scores) / len(relevance_scores)\n",
    "    print(f\"Average ROUGE-L F1 Score: {av_score}\")\n",
    "    return av_score\n",
    "\n",
    "def test_summary_002():\n",
    "    av_score = get_relevance_score()\n",
    "    assert av_score > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313fb47-185c-4bd2-9515-c9011215713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784a883-c72c-4e9a-8326-f91a347d62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Fourth test case\n",
    "####Tone and style, Customer service, LLM Based Likert scale, LLM as a Judge\n",
    "\n",
    "inquiries = [\n",
    "    {\"text\": \"This is the third time you've messed up my order. I want a refund NOW!\", \"tone\": \"empathetic\"},  # Edge case: Angry customer\n",
    "    {\"text\": \"I tried resetting my password but then my account got locked...\", \"tone\": \"patient\"},  # Edge case: Complex issue\n",
    "    {\"text\": \"I can't believe how good your product is. It's ruined all others for me!\", \"tone\": \"professional\"},  # Edge case: Compliment as complaint\n",
    "    # ... 97 more inquiries\n",
    "]\n",
    "\n",
    "def evaluate_likert(model_output, target_tone):\n",
    "    tone_prompt = f\"\"\"Rate this customer service response on a scale of 1-5 for being {target_tone}:\n",
    "    <response>{model_output}</response>\n",
    "    1: Not at all {target_tone}\n",
    "    5: Perfectly {target_tone}\n",
    "    Output only the number.\"\"\"\n",
    "\n",
    "    # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output \n",
    "    #response = client.messages.create(model=\"claude-sonnet-4-20250514\", max_tokens=50, messages=[{\"role\": \"user\", \"content\": tone_prompt}])\n",
    "    response: ChatResponse = chat(model='gpt-oss:20b', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': tone_prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content'].strip()\n",
    "\n",
    "def get_evaluations():\n",
    "    outputs = [chat_response(f\"Respond to this customer inquiry: {inquiry['text']}\") for inquiry in inquiries]\n",
    "    tone_scores = [evaluate_likert(output, inquiry['tone']) for output, inquiry in zip(outputs, inquiries)]\n",
    "    av_score = sum(tone_scores) / len(tone_scores)\n",
    "    print(f\"Average Tone Score: {av_score}\")\n",
    "    return av_score\n",
    "\n",
    "def test_customer_enquiry_002():\n",
    "    av_score = get_evaluations()\n",
    "    assert av_score > 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67dade0-2d45-4bbc-971d-2f98b10a7783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
